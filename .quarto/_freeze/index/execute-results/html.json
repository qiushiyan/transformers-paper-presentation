{
  "hash": "62da443bf1383e199a087acb8cc59f8e",
  "result": {
    "markdown": "## The 3 papers series in Word2Vec\n\n- Efficient Estimation of Word Representations in Vector Space (Jan 2013)\n\n- [Distributed Representations of Words and Phrases and their Compositionality]{.yellow} (Oct 2013)\n\n- Enriching Word Vectors with Subword Information (2016)\n\n## What was achieved then\n\nSkip-Gram works well on datasets of various sizes, and can better represent less frequent words.\n\n$$\n\\frac{1}{T} \\sum_{t=1}^T \\sum_{-c \\leq j \\leq c, j \\neq 0} \\log p(w_{t+j} | w_t)\n$$\n\n\n::: {.aside}\n\n- T: the number of words\n\n- c: the context window size\n\n- p: the softmax based probability $p(w_i | w) = \\frac{\\exp(v_w^Tw)}{\\sum_{w=1}^W \\exp(v_w^Tw)}$\n\n\n:::\n\nFor big datasets, the estimation is more efficient than previous neural network based methods.\n\nIt also work well with small datasets due to less parameters.\n\n##\n\n![](https://www.baeldung.com/wp-content/uploads/sites/4/2021/03/Screenshot-2021-03-05-at-11.29.31-1024x616-1.png)\n\n## The problem\n\n<!-- * worse representation of frequent words (compared to CBOW) -->\n\n1. Performance is not optimal because full softmax is hard to compute\n\n$$\np(w_t | w) = \\frac{\\exp(w_t^Tw)}{\\color{yellow}{\\sum_{t=1}^T \\exp(w_t^Tw)}}\n$$\n\n\n2. Inability to represent idiomatic phrases\n\n  - i.e., vec(\"Air\") + vec(\"Canada\") is far away from vec(\"Air Canada\")\n\n::: {.fragment .fade-up}\n\n<hr />\n\n1. use approximation with less complexity\n\n3. train on phrase vectors\n\n:::\n\n\n\n## Solution for performance\n\n- Negative Sampling\n\n  - manually create negative samples (k < 20) for each context-target pair and fit a logistic regression model to find the weights\n\n- Hierarchical Softmax\n  - each word is represented by a node in a binary tree, random walk is used to assign proababilities for each node, thus the complexity is reduced to $O(log(W))$\n\n- Subsampling for frequent words\n\n  - each word in the corpus is discarded with the probability\n\n  $$\n  p(w_i) = 1 - \\sqrt{\\frac{t}{f(w_i)}}\n  $$\n\n\n::: {.aside}\n\nt: constant threshold, usually 10e-5\n\n:::\n\n## Algorithm for negative sampling\n\n:::: {.columns}\n\n::: {.column width=\"55%\"}\n```{.python style=\"font-size: 1.1em\"}\nfor word in context_window:\n  s_1 = (word, target, 1)\n  for i in range(k):\n    random_word = sample(all_words)\n    if random_word != target:\n      s_i = (word, random_word, 0)\n\n  logistic_regression([s_1, s_2, ..., s_k])\n```\n\nThe sampling distribution is given by\n\n$$\nP(w_i) = \\frac{f(w_i)^{3/4}}{\\sum_{w=1}^W f(w)^{3/4}}\n$$\n\n\n:::\n\n::: {.column width=\"40%\"}\n\nI [had]{.blue} [milk]{.yellow} for breakfast.\n\ncontext | target | label\n--- | --- | ---\nhad | milk | 1\nhad | breakfast | 0\nhad | the | 0\nhad | I | 0\nhad | ... | 0\n\n:::\n\n::: {.aside}\n\n[context]{.blue}\n\n[target]{.yellow}\n\n:::\n\n::::\n\n## Solution for representing phrases\n\nPhrases are represented as unique tokens in the training set. Phrases are identified using\n\n$$\n\\text{score}(w_i, w_j) = \\frac{\\text{count}(w_i, w_j) - \\delta}{\\text{count}(w_i) \\times \\text{count}(w_j)}\n$$\n\n\n![Vector compositionality using element-wise addition. Four closest tokens to the sum of two\nvectors are shown, using the best Skip-gram model.](composability.png)\n\n::: {.aside}\n\n$\\delta$ is used as a discounting coefficient and prevents too many phrases consisting of very infrequent words to be formed\n\n:::\n\n## Questions\n\n::: {.fragment}\n\n1. Negative sampling is used to create supervised data when the original one is too sparse, can you think of other application domains?\n\n:::\n\n::: {.fragment}\n\n2. We can possibly use the empirical distribution or uniform distribution to sample negative samples, what might be the problem?\n\nRecap on the true sampling distribution\n    $$\n    P(w_i) = \\frac{f(w_i)^{3/4}}{\\sum_{w=1}^W f(w)^{3/4}}\n    $$\n:::\n\n\n## What could be (and actually was) improved\n\nThe third Word2Vec paper expanded on the idea of capturing subword information.\n\nEach word is represented as a bag of character n-grams.\n\nA vector representation is associated to each character n-gram; words being represented as the sum of these representations.\n\n## Demo\n\n\n::: {.cell output-location='slide'}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-1-1.png){width=960}\n:::\n:::\n\n\n\n## Additional resources\n\n- [Word2Vec Code Walkthrough with gensim](https://gist.github.com/aparrish/2f562e3737544cf29aaf1af30362f469)\n\n- [Negative Sampling](https://www.youtube.com/watch?v=4PXILCmVK4Q)",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    function fireSlideChanged(previousSlide, currentSlide) {\n\n      // dispatch for htmlwidgets\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for reveal\n    if (window.Reveal) {\n      window.Reveal.addEventListener(\"slidechanged\", function(event) {\n        fireSlideChanged(event.previousSlide, event.currentSlide);\n      });\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}