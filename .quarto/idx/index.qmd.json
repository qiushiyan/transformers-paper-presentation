{"title":"Paper Presentation","markdown":{"headingText":"The 3 papers series in Word2Vec","containsRefs":false,"markdown":"\n- Efficient Estimation of Word Representations in Vector Space (Jan 2013)\n\n- [Distributed Representations of Words and Phrases and their Compositionality]{.yellow} (Oct 2013)\n\n- Enriching Word Vectors with Subword Information (2016)\n\n## What was achieved then\n\nSkip-Gram works well on datasets of various sizes, and can better represent less frequent words.\n\n$$\n\\frac{1}{T} \\sum_{t=1}^T \\sum_{-c \\leq j \\leq c, j \\neq 0} \\log p(w_{t+j} | w_t)\n$$\n\n\n::: {.aside}\n\n- T: the number of words\n\n- c: the context window size\n\n- p: the softmax based probability $p(w_i | w) = \\frac{\\exp(v_w^Tw)}{\\sum_{w=1}^W \\exp(v_w^Tw)}$\n\n\n:::\n\nFor big datasets, the estimation is more efficient than previous neural network based methods.\n\nIt also work well with small datasets due to less parameters.\n\n##\n\n![](https://www.baeldung.com/wp-content/uploads/sites/4/2021/03/Screenshot-2021-03-05-at-11.29.31-1024x616-1.png)\n\n## The problem\n\n<!-- * worse representation of frequent words (compared to CBOW) -->\n\n1. Performance is not optimal because full softmax is hard to compute\n\n$$\np(w_t | w) = \\frac{\\exp(w_t^Tw)}{\\color{yellow}{\\sum_{t=1}^T \\exp(w_t^Tw)}}\n$$\n\n\n2. Inability to represent idiomatic phrases\n\n  - i.e., vec(\"Air\") + vec(\"Canada\") is far away from vec(\"Air Canada\")\n\n::: {.fragment .fade-up}\n\n<hr />\n\n1. use approximation with less complexity\n\n3. train on phrase vectors\n\n:::\n\n\n\n## Solution for performance\n\n- Negative Sampling\n\n  - manually create negative samples (k < 20) for each context-target pair and fit a logistic regression model to find the weights\n\n- Hierarchical Softmax\n  - each word is represented by a node in a binary tree, random walk is used to assign proababilities for each node, thus the complexity is reduced to $O(log(W))$\n\n- Subsampling for frequent words\n\n  - each word in the corpus is discarded with the probability\n\n  $$\n  p(w_i) = 1 - \\sqrt{\\frac{t}{f(w_i)}}\n  $$\n\n\n::: {.aside}\n\nt: constant threshold, usually 10e-5\n\n:::\n\n## Algorithm for negative sampling\n\n:::: {.columns}\n\n::: {.column width=\"55%\"}\n```{.python style=\"font-size: 1.1em\"}\nfor word in context_window:\n  s_1 = (word, target, 1)\n  for i in range(k):\n    random_word = sample(all_words)\n    if random_word != target:\n      s_i = (word, random_word, 0)\n\n  logistic_regression([s_1, s_2, ..., s_k])\n```\n\nThe sampling distribution is given by\n\n$$\nP(w_i) = \\frac{f(w_i)^{3/4}}{\\sum_{w=1}^W f(w)^{3/4}}\n$$\n\n\n:::\n\n::: {.column width=\"40%\"}\n\nI [had]{.blue} [milk]{.yellow} for breakfast.\n\ncontext | target | label\n--- | --- | ---\nhad | milk | 1\nhad | breakfast | 0\nhad | the | 0\nhad | I | 0\nhad | ... | 0\n\n:::\n\n::: {.aside}\n\n[context]{.blue}\n\n[target]{.yellow}\n\n:::\n\n::::\n\n## Solution for representing phrases\n\nPhrases are represented as unique tokens in the training set. Phrases are identified using\n\n$$\n\\text{score}(w_i, w_j) = \\frac{\\text{count}(w_i, w_j) - \\delta}{\\text{count}(w_i) \\times \\text{count}(w_j)}\n$$\n\n\n![Vector compositionality using element-wise addition. Four closest tokens to the sum of two\nvectors are shown, using the best Skip-gram model.](composability.png)\n\n::: {.aside}\n\n$\\delta$ is used as a discounting coefficient and prevents too many phrases consisting of very infrequent words to be formed\n\n:::\n\n## Questions\n\n::: {.fragment}\n\n1. Negative sampling is used to create supervised data when the original one is too sparse, can you think of other application domains?\n\n:::\n\n::: {.fragment}\n\n2. We can possibly use the empirical distribution or uniform distribution to sample negative samples, what might be the problem?\n\nRecap on the true sampling distribution\n    $$\n    P(w_i) = \\frac{f(w_i)^{3/4}}{\\sum_{w=1}^W f(w)^{3/4}}\n    $$\n:::\n\n\n## What could be (and actually was) improved\n\nThe third Word2Vec paper expanded on the idea of capturing subword information.\n\nEach word is represented as a bag of character n-grams.\n\nA vector representation is associated to each character n-gram; words being represented as the sum of these representations.\n\n## Demo\n\n```python\nif not os.path.exists(\"GoogleNews-vectors-negative300.bin\"):\n    !wget -c -nc \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n    !gunzip GoogleNews-vectors-negative300.bin.gz\n\nkeys = ['Paris', 'Python', 'Sunday', 'Tolstoy', 'Twitter', 'bachelor', 'delivery', 'election', 'expensive',\n    'experience', 'financial', 'food', 'iOS', 'peace', 'release', 'war']\n\nembedding_clusters = []\nword_clusters = []\nfor word in keys:\n    print(f\"Key = {word}\")\n    embeddings = []\n    words = []\n    for similar_word, _ in model_gn.most_similar(word, topn=30):\n        words.append(similar_word)\n        embeddings.append(model_gn[similar_word])\n    embedding_clusters.append(embeddings)\n    word_clusters.append(words)\n\ndef tsne_plot_similar_words(title, labels, embedding_clusters, word_clusters, a, filename=None):\n  figsize = (9.5,6) if (matplotlib.get_backend() == 'nbAgg') else (20,12)  # interactive plot should be smaller\n  plt.figure(figsize=(figsize))\n  colors = cm.rainbow(np.linspace(0, 1, len(labels)))\n  for label, embeddings, words, color in zip(labels, embedding_clusters, word_clusters, colors):\n      x = embeddings[:, 0]\n      y = embeddings[:, 1]\n      plt.scatter(x, y, c=[color], alpha=a, label=label)\n      for i, word in enumerate(words):\n          plt.annotate(word, alpha=0.5, xy=(x[i], y[i]), xytext=(5, 2),\n                        textcoords='offset points', ha='right', va='bottom', size=8)\n  plt.legend(loc=4)\n  plt.title(title)\n  plt.grid(True)\n  plt.show()\n\n\ntsne_plot_similar_words('Similar words from Google News', keys, embeddings_en_2d, word_clusters, 0.7,\n                        'similar_words.png')\n```\n\n##\n\n![](cluster.png)\n\n## Additional resources\n\n- [Word2Vec Code Walkthrough with gensim](https://gist.github.com/aparrish/2f562e3737544cf29aaf1af30362f469)\n\n- [Gensim Word2Vec doc](https://radimrehurek.com/gensim/models/word2vec.html)\n\n- [Negative Sampling](https://www.youtube.com/watch?v=4PXILCmVK4Q)\n"},"formats":{"revealjs":{"execute":{"fig-width":10,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":false,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"wrap","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","html-math-method":{"method":"mathjax","url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML-full"},"slide-level":2,"to":"revealjs","filters":["shinylive"],"output-file":"index.html"},"language":{},"metadata":{"lang":"en","fig-responsive":false,"quarto-version":"1.2.198","auto-stretch":true,"title":"Paper Presentation","author":"Qiushi Yan <br /> [qiushiyan/transformers-paper-presentation](https://github.com/qiushiyan/transformers-paper-presentation)","jupyter":"python3.10","theme":["default","custom.scss"],"date":"now","scrollable":true,"slideNumber":true,"chalkboard":true}}}}